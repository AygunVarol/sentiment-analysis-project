{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Technical Assignment Week 3\n\n# Ayg√ºn Varol\n\n## aygun.varol@tuni.fi","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Download the IMDB Dataset (1 point) ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:20:34.281411Z","iopub.execute_input":"2025-02-04T12:20:34.281846Z","iopub.status.idle":"2025-02-04T12:20:34.287493Z","shell.execute_reply.started":"2025-02-04T12:20:34.281818Z","shell.execute_reply":"2025-02-04T12:20:34.286665Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import the Pandas library\nimport pandas as pd\n\n# Define the path to the dataset\ndata_path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n\n# Load the dataset into a DataFrame\ndf = pd.read_csv(data_path)\n\n# Verify the dataset by displaying the first few rows and some info\nprint(\"First 5 rows of the dataset:\")\nprint(df.head())\n\nprint(\"\\nDataset Information:\")\nprint(df.info())\n\nprint(\"\\nDataset Shape:\")\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:20:36.023413Z","iopub.execute_input":"2025-02-04T12:20:36.023806Z","iopub.status.idle":"2025-02-04T12:20:37.675259Z","shell.execute_reply.started":"2025-02-04T12:20:36.023772Z","shell.execute_reply":"2025-02-04T12:20:37.674402Z"}},"outputs":[{"name":"stdout","text":"First 5 rows of the dataset:\n                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n\nDataset Information:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   review     50000 non-null  object\n 1   sentiment  50000 non-null  object\ndtypes: object(2)\nmemory usage: 781.4+ KB\nNone\n\nDataset Shape:\n(50000, 2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 2: Data Preprocessing (1 point)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Assuming the dataset has been loaded into a DataFrame `df` as shown in Step 1.\n# For example:\n# df = pd.read_csv('/kaggle/input/imdb-dataset/IMDB Dataset.csv')\n\n# 1. Clean and preprocess the dataset:\n\n# a. Encode the sentiment column: positive -> 1, negative -> 0\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# b. Retain only the review and sentiment columns.\n#    Optionally, rename the sentiment column to \"label\" for clarity.\ndf = df[['review', 'sentiment']]\ndf.rename(columns={'sentiment': 'label'}, inplace=True)\n\n# Display the first few rows to verify the changes\nprint(\"Preprocessed Data:\")\nprint(df.head())\n\n# 2. Split the data into training, validation, and testing sets.\n\n# First, split the data into training (80%) and testing (20%) sets.\n# Using stratify ensures that the distribution of labels is maintained in both splits.\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\n# Next, split the training data into training and validation sets.\n# Here, we take 12.5% of the training data as validation which is equivalent to 10% of the total dataset \n# (0.125 * 80% = 10%).\ntrain_data, val_data = train_test_split(train_data, test_size=0.125, random_state=42, stratify=train_data['label'])\n\n# Verify the splits by printing out the shapes\nprint(\"\\nData Split Shapes:\")\nprint(f\"Training set shape: {train_data.shape}\")\nprint(f\"Validation set shape: {val_data.shape}\")\nprint(f\"Test set shape: {test_data.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:20:40.426729Z","iopub.execute_input":"2025-02-04T12:20:40.427040Z","iopub.status.idle":"2025-02-04T12:20:41.000685Z","shell.execute_reply.started":"2025-02-04T12:20:40.427015Z","shell.execute_reply":"2025-02-04T12:20:40.999923Z"}},"outputs":[{"name":"stdout","text":"Preprocessed Data:\n                                              review  label\n0  One of the other reviewers has mentioned that ...      1\n1  A wonderful little production. <br /><br />The...      1\n2  I thought this was a wonderful way to spend ti...      1\n3  Basically there's a family where a little boy ...      0\n4  Petter Mattei's \"Love in the Time of Money\" is...      1\n\nData Split Shapes:\nTraining set shape: (35000, 2)\nValidation set shape: (5000, 2)\nTest set shape: (10000, 2)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step 3: Model Selection and Tokenization (1 point)","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\n# 1. Select a pre-trained Hugging Face transformer model\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Assuming you have the following Pandas DataFrames from previous steps:\n#   - train_data\n#   - val_data\n#   - test_data\n# Each DataFrame contains two columns: \"review\" and \"label\".\n\n# Optionally, convert your Pandas DataFrames to Hugging Face Dataset objects:\ntrain_dataset = Dataset.from_pandas(train_data)\nval_dataset   = Dataset.from_pandas(val_data)\ntest_dataset  = Dataset.from_pandas(test_data)\n\n# 2. Tokenize the dataset with truncation, padding, and max_length set to 256.\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"review\"],\n        truncation=True,          # Enable truncation to avoid sequences longer than max_length\n        padding=\"max_length\",       # Pad sequences to the maximum length\n        max_length=256              # Set maximum sequence length\n    )\n\n# Apply the tokenization to each dataset split.\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset   = val_dataset.map(tokenize_function, batched=True)\ntest_dataset  = test_dataset.map(tokenize_function, batched=True)\n\n# Optionally, remove the original text column if you don't need it for training.\ntrain_dataset = train_dataset.remove_columns([\"review\"])\nval_dataset   = val_dataset.remove_columns([\"review\"])\ntest_dataset  = test_dataset.remove_columns([\"review\"])\n\n# If you're planning to use the Hugging Face Trainer for fine-tuning, it's common practice to rename the 'label' column to 'labels'\ntrain_dataset = train_dataset.rename_column(\"label\", \"labels\")\nval_dataset   = val_dataset.rename_column(\"label\", \"labels\")\ntest_dataset  = test_dataset.rename_column(\"label\", \"labels\")\n\n# Finally, set the format for PyTorch (or TensorFlow, if needed)\ntrain_dataset.set_format(\"torch\")\nval_dataset.set_format(\"torch\")\ntest_dataset.set_format(\"torch\")\n\n# Verify tokenization by checking an example from the training dataset.\nprint(\"Tokenized example from training dataset:\")\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:21:09.172151Z","iopub.execute_input":"2025-02-04T12:21:09.172468Z","iopub.status.idle":"2025-02-04T12:21:33.678080Z","shell.execute_reply.started":"2025-02-04T12:21:09.172443Z","shell.execute_reply":"2025-02-04T12:21:33.677245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ebfb47533c4414bf382a9744fbbf03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0efc7986e064b66b4fef3e070cdf085"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71fd04d81a2f4c84a3319f6b1eb074cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70e3c802b26432c8a22c290b124bc56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f2daa86f7b4cb59705299adb34b5b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21452dac13b342b0805e2529a190bb7f"}},"metadata":{}},{"name":"stdout","text":"Tokenized example from training dataset:\n{'labels': tensor(0), '__index_level_0__': tensor(4427), 'input_ids': tensor([  101,  2002,  2428,  2439,  1996,  5436,  2007,  2023,  2028,   999,\n         3904,  1997,  2010,  8200, 11749,  2015,  2182,  2012,  2035,  1010,\n         2019,  4895, 18447, 18702,  3436,  5436,  1998,  3294,  6659,  3772,\n         2191,  2023,  2010,  5409,  2143,  1006,  1999,  2026,  5448,  1007,\n         1012,  2130,  2010, 11749, 13638,  2003,  2908,  1010,  3347,  2028,\n         3496,  1999,  2019,  4082,  3004,  1012,  2821,  2092,  1010,  2012,\n         2560,  2010,  2279,  2143,  1005, 10103,  4164,  1005,  3662,  2008,\n         2002,  2071,  2145,  5213,  2043,  2002,  2359,  2000,  1012,  1012,\n         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 4: Fine-Tune the Model (2 points)","metadata":{}},{"cell_type":"code","source":"# Disable wandb integration\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Import necessary libraries\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# 1. Load a pre-trained model for sequence classification.\n#    Since the IMDB dataset has two labels (positive and negative), we set num_labels=2.\nmodel_name = \"distilbert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# 2. Define a function to compute evaluation metrics.\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_score(labels, predictions)\n    # Use weighted average to account for any label imbalance.\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\n# 3. Define training parameters with TrainingArguments.\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name=\"imdb-finetuning-run\",\n    eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n# 4. Create a Trainer instance.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,         # Preprocessed and tokenized training dataset\n    eval_dataset=val_dataset,             # Preprocessed and tokenized validation dataset\n    compute_metrics=compute_metrics,      # Metrics function to compute accuracy, precision, recall, and F1-score\n)\n\n# 5. Fine-tune the model.\ntrainer.train()\n\n# Optionally, evaluate the model after training.\neval_results = trainer.evaluate()\nprint(\"Evaluation Results:\", eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:26:47.152020Z","iopub.execute_input":"2025-02-04T12:26:47.152380Z","iopub.status.idle":"2025-02-04T12:34:44.451041Z","shell.execute_reply.started":"2025-02-04T12:26:47.152317Z","shell.execute_reply":"2025-02-04T12:34:44.450302Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2188' max='2188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2188/2188 07:38, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.197100</td>\n      <td>0.213713</td>\n      <td>0.917600</td>\n      <td>0.917622</td>\n      <td>0.917600</td>\n      <td>0.917599</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 00:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.21371318399906158, 'eval_accuracy': 0.9176, 'eval_precision': 0.9176216495063105, 'eval_recall': 0.9176, 'eval_f1': 0.9175989320821598, 'eval_runtime': 17.682, 'eval_samples_per_second': 282.774, 'eval_steps_per_second': 17.702, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Step 5: Save and Upload the Model to Hugging Face (2 points)","metadata":{}},{"cell_type":"code","source":"# 1. Save the fine-tuned model and tokenizer locally.\nsave_directory = \"./finetuned-distilbert-imdb\"\n\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model and tokenizer saved locally in {save_directory}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:35:24.436861Z","iopub.execute_input":"2025-02-04T12:35:24.437177Z","iopub.status.idle":"2025-02-04T12:35:25.092686Z","shell.execute_reply.started":"2025-02-04T12:35:24.437153Z","shell.execute_reply":"2025-02-04T12:35:25.091866Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved locally in ./finetuned-distilbert-imdb\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# 2. Log in to Hugging Face using notebook_login.\n# This will prompt you to enter your Hugging Face access token.\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:36:53.495098Z","iopub.execute_input":"2025-02-04T12:36:53.495436Z","iopub.status.idle":"2025-02-04T12:36:53.512705Z","shell.execute_reply.started":"2025-02-04T12:36:53.495405Z","shell.execute_reply":"2025-02-04T12:36:53.511790Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f5b9909e4244192b80298674767742b"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# 3. Upload the model and tokenizer to Hugging Face.\n# Choose a repository name. The repository name should be unique and is typically in the format \"username/repo_name\".\n# Replace 'your-username' and 'finetuned-distilbert-imdb' with your actual username and desired repo name.\nrepo_name = \"Aygun/finetuned-distilbert-imdb\"\n\n# Push the model and tokenizer to the hub.\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)\n\nprint(f\"Model and tokenizer have been pushed to the Hugging Face Hub at: https://huggingface.co/{repo_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:38:13.512121Z","iopub.execute_input":"2025-02-04T12:38:13.512447Z","iopub.status.idle":"2025-02-04T12:38:34.681892Z","shell.execute_reply.started":"2025-02-04T12:38:13.512420Z","shell.execute_reply":"2025-02-04T12:38:34.680963Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcaf5fb4c5ab44bd8f9d3cc91e721cbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d42875c568149e6a2800a540208dd2f"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer have been pushed to the Hugging Face Hub at: https://huggingface.co/Aygun/finetuned-distilbert-imdb\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Part 2: API Development and Testing (5 points) ","metadata":{}},{"cell_type":"markdown","source":"## Step 6: Set Up the Backend API (1 point) ","metadata":{}},{"cell_type":"code","source":"from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport random\n\n# Initialize the FastAPI app\napp = FastAPI()\n\n# Define the request schema using Pydantic\nclass SentimentRequest(BaseModel):\n    text: str\n    model: str  # Expected values: \"custom\" or \"llama\"\n\n# Define the response schema using Pydantic\nclass SentimentResponse(BaseModel):\n    sentiment: str        # \"positive\" or \"negative\"\n    confidence: float     # Confidence score (e.g., 0.95)\n\ndef analyze_sentiment(text: str, model: str) -> (str, float):\n    \"\"\"\n    Dummy sentiment analysis function.\n    Replace this logic with calls to your actual sentiment analysis models.\n    \"\"\"\n    # Validate model parameter\n    if model not in [\"custom\", \"llama\"]:\n        raise ValueError(\"Invalid model specified. Please use 'custom' or 'llama'.\")\n\n    # Simple heuristic for demonstration:\n    # If the text contains the word \"good\", we assume it's positive.\n    # Otherwise, we assume it's negative.\n    if \"good\" in text.lower():\n        sentiment = \"positive\"\n        confidence = round(random.uniform(0.80, 1.0), 2)\n    else:\n        sentiment = \"negative\"\n        confidence = round(random.uniform(0.60, 0.79), 2)\n    \n    return sentiment, confidence\n\n@app.post(\"/analyze/\", response_model=SentimentResponse)\ndef analyze(request: SentimentRequest):\n    \"\"\"\n    POST endpoint to analyze sentiment.\n    - **text**: The input text to analyze.\n    - **model**: The model to use (\"custom\" or \"llama\").\n    Returns a JSON response with the sentiment and confidence score.\n    \"\"\"\n    try:\n        sentiment, confidence = analyze_sentiment(request.text, request.model)\n        return SentimentResponse(sentiment=sentiment, confidence=confidence)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n# To run the app, use the command: uvicorn step9:app --reload\n# uvicorn step6:app --reload","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:55:37.174958Z","iopub.execute_input":"2025-02-04T12:55:37.175261Z","iopub.status.idle":"2025-02-04T12:55:37.185758Z","shell.execute_reply.started":"2025-02-04T12:55:37.175236Z","shell.execute_reply":"2025-02-04T12:55:37.184979Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Step 7: Load Models (1 point)","metadata":{}},{"cell_type":"code","source":"#  Load the Fine-Tuned Model from Hugging Face\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Hugging Face Hub\nmodel_repo = \"Aygun/finetuned-distilbert-imdb\"\n\n# Load the fine-tuned model and tokenizer from Hugging Face\nmodel = AutoModelForSequenceClassification.from_pretrained(model_repo)\ntokenizer = AutoTokenizer.from_pretrained(model_repo)\n\nprint(\"Fine-tuned model and tokenizer loaded from Hugging Face.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:56:02.958906Z","iopub.execute_input":"2025-02-04T12:56:02.959208Z","iopub.status.idle":"2025-02-04T12:56:11.249620Z","shell.execute_reply.started":"2025-02-04T12:56:02.959181Z","shell.execute_reply":"2025-02-04T12:56:11.248753Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a96119848ef94f24b7df9d03cb0c8031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a5f26456d1427f99bf59f47c0453c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fcc8b423df04777ac0db7acf97f14ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a97fe8ba91b94c3498e8c3560334979d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41ab4ac74a342859d936d40306541c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f81d41a5046544c8a52fdd99e2ff0e82"}},"metadata":{}},{"name":"stdout","text":"Fine-tuned model and tokenizer loaded from Hugging Face.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"! pip install groq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T13:06:22.301169Z","iopub.execute_input":"2025-02-04T13:06:22.301496Z","iopub.status.idle":"2025-02-04T13:06:25.868029Z","shell.execute_reply.started":"2025-02-04T13:06:22.301467Z","shell.execute_reply":"2025-02-04T13:06:25.866929Z"}},"outputs":[{"name":"stdout","text":"Collecting groq\n  Downloading groq-0.17.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\nDownloading groq-0.17.0-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: groq\nSuccessfully installed groq-0.17.0\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"#set GROQ_API_KEY in the secrets\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GROQ_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T13:17:44.637663Z","iopub.execute_input":"2025-02-04T13:17:44.637987Z","iopub.status.idle":"2025-02-04T13:17:44.794549Z","shell.execute_reply.started":"2025-02-04T13:17:44.637956Z","shell.execute_reply":"2025-02-04T13:17:44.793811Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"import os\nfrom groq import Groq\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n\n# Create the Groq client\nclient = Groq(api_key=api_key)\n\n# Set system prompt\nsystem_prompt = {\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant. You reply with very short answers.\"\n}\n\n# Initialize chat history\nchat_history = [system_prompt]\n\nwhile True:\n    user_input = input(\"You: \")\n    \n    if user_input.lower() in [\"exit\", \"quit\"]:\n        print(\"Exiting chat.\")\n        break\n    \n    chat_history.append({\"role\": \"user\", \"content\": user_input})\n\n    response = client.chat.completions.create(\n        model=\"llama3-70b-8192\",\n        messages=chat_history,\n        max_tokens=100,\n        temperature=1.2\n    )\n\n    assistant_reply = response.choices[0].message.content\n    chat_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n\n    print(\"Assistant:\", assistant_reply)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T13:19:29.626947Z","iopub.execute_input":"2025-02-04T13:19:29.627288Z","iopub.status.idle":"2025-02-04T13:19:37.000772Z","shell.execute_reply.started":"2025-02-04T13:19:29.627263Z","shell.execute_reply":"2025-02-04T13:19:37.000018Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"You:  Hello how are you?\n"},{"name":"stdout","text":"Assistant: I'm good, thanks!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"Exiting chat.\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"## Step 8: Test the API Locally (1 point)","metadata":{}},{"cell_type":"code","source":"! pip install reactpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:53:45.832700Z","iopub.execute_input":"2025-02-04T14:53:45.832987Z","iopub.status.idle":"2025-02-04T14:53:50.890871Z","shell.execute_reply.started":"2025-02-04T14:53:45.832966Z","shell.execute_reply":"2025-02-04T14:53:50.889752Z"}},"outputs":[{"name":"stdout","text":"Collecting reactpy\n  Downloading reactpy-1.1.0-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: anyio>=3 in /usr/local/lib/python3.10/dist-packages (from reactpy) (3.7.1)\nCollecting asgiref>=3 (from reactpy)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: colorlog>=6 in /usr/local/lib/python3.10/dist-packages (from reactpy) (6.9.0)\nRequirement already satisfied: exceptiongroup>=1.0 in /usr/local/lib/python3.10/dist-packages (from reactpy) (1.2.2)\nRequirement already satisfied: fastjsonschema>=2.14.5 in /usr/local/lib/python3.10/dist-packages (from reactpy) (2.21.1)\nRequirement already satisfied: jsonpatch>=1.32 in /usr/local/lib/python3.10/dist-packages (from reactpy) (1.33)\nRequirement already satisfied: lxml>=4 in /usr/local/lib/python3.10/dist-packages (from reactpy) (5.3.0)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from reactpy) (1.0.0)\nRequirement already satisfied: requests>=2 in /usr/local/lib/python3.10/dist-packages (from reactpy) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.10 in /usr/local/lib/python3.10/dist-packages (from reactpy) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio>=3->reactpy) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3->reactpy) (1.3.1)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch>=1.32->reactpy) (3.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2->reactpy) (3.4.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2->reactpy) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2->reactpy) (2024.12.14)\nDownloading reactpy-1.1.0-py3-none-any.whl (110 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.3/110.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nInstalling collected packages: asgiref, reactpy\nSuccessfully installed asgiref-3.8.1 reactpy-1.1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import requests\n\n# URL of the API endpoint\nurl = \"http://127.0.0.1:8000/analyze/\"\n\n# Example payload for the custom model\npayload_custom = {\n    \"text\": \"This movie was fantastic!\",\n    \"model\": \"custom\"\n}\n\n# Example payload for the llama model\npayload_llama = {\n    \"text\": \"This movie was terrible.\",\n    \"model\": \"llama\"\n}\n\nheaders = {\"Content-Type\": \"application/json\"}\n\n# Test with custom model\nresponse_custom = requests.post(url, json=payload_custom, headers=headers)\nprint(\"Custom Model Response:\")\nprint(response_custom.json())\n\n# Test with llama model\nresponse_llama = requests.post(url, json=payload_llama, headers=headers)\nprint(\"Llama Model Response:\")\nprint(response_llama.json())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Define the Llama 3 Prompt (1 point) ","metadata":{}},{"cell_type":"markdown","source":"### \"Please analyze the sentiment of the following text and classify it as either 'positive' or 'negative'. Text: '{input_text}'. Provide your answer in the format: Sentiment: [positive/negative], Confidence: [percentage].\"","metadata":{}},{"cell_type":"markdown","source":"## Step 10: Test with Both Models (1 point)","metadata":{}},{"cell_type":"code","source":"import requests\n\n# URL of the locally running FastAPI service\nurl = \"http://127.0.0.1:8000/analyze/\"\n\n# Payload for the custom (fine-tuned) model\npayload_custom = {\n    \"text\": \"This movie was fantastic!\",\n    \"model\": \"custom\"  # This will trigger your fine-tuned Hugging Face model\n}\n\n# Payload for the Llama 3 model via Groq Cloud\npayload_llama = {\n    \"text\": \"This movie was terrible.\",\n    \"model\": \"llama\"   # This will trigger your Groq Cloud integration with Llama 3\n}\n\nheaders = {\"Content-Type\": \"application/json\"}\n\n# Test the custom model endpoint\nresponse_custom = requests.post(url, json=payload_custom, headers=headers)\nprint(\"Custom Model Response:\")\nprint(response_custom.json())\n\n# Test the Llama 3 model endpoint\nresponse_llama = requests.post(url, json=payload_llama, headers=headers)\nprint(\"Llama Model Response:\")\nprint(response_llama.json())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 3: UI Design and Explanation (3 points)","metadata":{}},{"cell_type":"markdown","source":"## Step 11: React UI Design  (1 point)","metadata":{}},{"cell_type":"code","source":"import reactpy\nfrom reactpy import component, html, hooks\nimport asyncio\nimport httpx  # For making asynchronous HTTP requests (optional)\n\n# Replace with your actual backend endpoint URL\nBACKEND_API_URL = \"http://localhost:8000/analyze\"\n\n@component\ndef SentimentAnalyzer():\n    # State for user input text\n    text_input, set_text_input = hooks.use_state(\"\")\n    # State for the selected model from the dropdown\n    selected_model, set_selected_model = hooks.use_state(\"Custom Model\")\n    # State for the result from the backend API\n    result, set_result = hooks.use_state(\"\")\n\n    # Handler for text input change\n    def handle_text_change(event):\n        set_text_input(event[\"target\"][\"value\"])\n\n    # Handler for dropdown selection change\n    def handle_model_change(event):\n        set_selected_model(event[\"target\"][\"value\"])\n\n    # Handler for button click that calls the backend API\n    async def analyze_sentiment(event):\n        if not text_input.strip():\n            set_result(\"Please enter some text to analyze.\")\n            return\n\n        # Create the payload to send to your API\n        payload = {\n            \"text\": text_input,\n            \"model\": selected_model,\n        }\n\n        try:\n            # Use an async HTTP client to post the data.\n            # Make sure your backend API is running and configured to accept these requests.\n            async with httpx.AsyncClient() as client:\n                response = await client.post(BACKEND_API_URL, json=payload)\n                response.raise_for_status()\n                data = response.json()\n                # Expected response structure:\n                # {\n                #    \"sentiment\": \"positive\" or \"negative\",\n                #    \"confidence\": 0.95  # optional\n                # }\n                sentiment = data.get(\"sentiment\", \"unknown\")\n                confidence = data.get(\"confidence\", None)\n                if confidence is not None:\n                    set_result(f\"Sentiment: {sentiment} (Confidence: {confidence:.2f})\")\n                else:\n                    set_result(f\"Sentiment: {sentiment}\")\n        except Exception as e:\n            set_result(f\"Error analyzing sentiment: {str(e)}\")\n\n    return html.div(\n        {\"style\": {\"fontFamily\": \"Arial, sans-serif\", \"maxWidth\": \"600px\", \"margin\": \"auto\", \"padding\": \"20px\"}},\n        html.h1(\"Sentiment Analyzer\"),\n        html.div(\n            {\"style\": {\"marginBottom\": \"10px\"}},\n            html.label({\"for\": \"text-input\"}, \"Enter text:\"),\n            html.input({\n                \"id\": \"text-input\",\n                \"type\": \"text\",\n                \"value\": text_input,\n                \"on_change\": handle_text_change,\n                \"style\": {\"width\": \"100%\", \"padding\": \"8px\", \"marginTop\": \"5px\"}\n            })\n        ),\n        html.div(\n            {\"style\": {\"marginBottom\": \"10px\"}},\n            html.label({\"for\": \"model-select\"}, \"Select model:\"),\n            html.select({\n                \"id\": \"model-select\",\n                \"value\": selected_model,\n                \"on_change\": handle_model_change,\n                \"style\": {\"width\": \"100%\", \"padding\": \"8px\", \"marginTop\": \"5px\"}\n            },\n                html.option({\"value\": \"Custom Model\"}, \"Custom Model\"),\n                html.option({\"value\": \"Llama 3\"}, \"Llama 3\")\n            )\n        ),\n        html.button({\n            \"on_click\": analyze_sentiment,\n            \"style\": {\"padding\": \"10px 20px\", \"cursor\": \"pointer\"}\n        }, \"Analyze Sentiment\"),\n        html.div(\n            {\"style\": {\"marginTop\": \"20px\", \"padding\": \"10px\", \"backgroundColor\": \"#f0f0f0\"}},\n            result\n        )\n    )\n\n# Run the ReactPy app\nif __name__ == \"__main__\":\n    reactpy.run(SentimentAnalyzer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 12: Submit GitHub Repository (1 point)","metadata":{}},{"cell_type":"markdown","source":"# https://github.com/AygunVarol/sentiment-analysis-project\n\n# https://huggingface.co/Aygun/finetuned-distilbert-imdb","metadata":{}},{"cell_type":"markdown","source":"## Step 13: Record a YouTube Demo Video (1 point)","metadata":{}},{"cell_type":"markdown","source":"# https://www.youtube.com/watch?v=9HHb9GtZIx4","metadata":{}}]}